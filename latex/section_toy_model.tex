\section{A Toy Model}
%
%
\begin{frame}
  So far we have worked out a sound theoretical foundation to understand the
  errors incurred when building learning algorithms.  In this section we will
  analyze in detail how these concepts look with a toy model.
\end{frame}
%
%
\begin{frame}
  Our data generating process will be very simple so that we can fully analyse
  the situation:

  \begin{align*}
    X &\sim U(0, 2 \pi) \\
    Y &\sim \sin(X) + N(0, \epsilon)
  \end{align*}

  Where $U$ is the uniform distribution on an interval, and $N$ is the normal
  distribution with a given mean and variance.
\end{frame}
%
%
\begin{frame}
  \begin{figure}
    \includegraphics[scale=0.09]{true_signal}
  \end{figure}

  Clearly, the regression function $E(Y \mid X)$ is given by:
  $$ \F(X) = E[ \sin(X) + N(0, \epsilon) \mid X ] = \sin(X) $$
\end{frame}
%
%
\begin{frame}
  The irreducible error component, which does not depend on our choice of
  learning algorithm, is easy to compute straight from the definition:

  \begin{align*}
      \IESE(x) &= E_Y \left[ \left( y - \F(x) \right)^2 \mid x \right] \\
      &= E_Y \left[ \left( \sin(X) + N(0, \epsilon) - \sin(X) \right)^2 \right] \\
      &= E_Y \left[ N(0, \epsilon)^2 \right] \\
      &= \epsilon
  \end{align*}
   
\end{frame}
%
%
\begin{frame}
  We take as our learning algorithm \textbf{linear regression}:

  $$ \LinReg: \D \mapsto \LinReg({\D}_{X}, {\D}_{Y}) $$

  \begin{figure}
    \includegraphics[scale=0.09]{single_fitted_line}
  \end{figure}
\end{frame}
%
%
\begin{frame}
  Let's study the bias of our toy model.  Recall the definition:

  \begin{align*}
    \BIAS (x)^2 = \left( \F(x) - Ef(x) \right)^2
  \end{align*}

  Where:

  \begin{align*}
    Ef(x) = E_{\D} \left[ \A(D) \right] \\ 
  \end{align*}

  Clearly, we need some way of determining $Ef$.
\end{frame}
%
%
\begin{frame}
  The only place to start is to unwind the definition.  The expectation is over
  the sampling distribution of $X, Y$.  We can think of sampling from this as a
  two step process:

  \begin{itemize}
    \item First take $N$ independent samples from $X \sim U(0, 2 \pi)$
    \item Given these samples $x_1, x_2, \ldots, x_N$, sample a value of $Y$
    from $Y \sim N(\sin(x_i), \epsilon)$, for each $x_i$
  \end{itemize}
\end{frame}
%
%
\begin{frame}
  \color{red}{\textbf{Warning}}: This is the most difficuly, mindbending
  mathematics in the talk.  Feel free to get coffee..
\end{frame}
%
%
\begin{frame}
  \begin{align*}
    E_{\D} & \left[ \A(D) \right] \\
    %
    =& \int_{S\D} \LinReg(\D) d\D \\
    %
    =& \int_{S(X,Y)} \LinReg(X, Y) dY dX \\
    %
    =& \int_{S(X,Y)} \argmin_{a,b} \left\{ \sum_i \left( Y_i - a X_i - b
    \right)^2 \right\} dY dX
    %
    =& \int_{S(X,Y)} \argmin_{a,b} \left\{ \sum_i \left( Y_i - a X_i - b
    \right)^2 \right\} dY dX
  \end{align*}
\end{frame}
