\section{The Out of Sample Error Rate}
%
%
\begin{frame}
  The goal in building a predictive model is often to find an
  appropriate functional form that minimizes the expected error
  rate:
  $$\ESE(f) = E \left[ \left( y - f(x) \right)^2 \right]$$
\end{frame}
%
%
\begin{frame}
  In this section, we turn our attention to estimating this quantity.
\end{frame}
%
%
\begin{frame}
  The first difficulty is that there are different ways to interpret the error,
  depending on what is or is not considered at random.
\end{frame}
%
%
\begin{frame}
  We have already discussed the error when ranodmizing over both a training set
  and an out of training sample, called the \textbf{expected out of sample
  error}:

  $$\ESE = E_{X,Y,\D} \left[ \left( Y - f(X; D) \right)^2 \right]$$

  It is this quantity that we were able to decompose with the
  \textbf{bias-varaince} decomposition.
\end{frame}
%
%
\begin{frame}
  Possibly more relevant, is the error when randomizing over an out of training
  sample, but for a \textit{fixed} training set:

  $$\ESE(\T) = E_{X,Y} \left[ \left( Y - f(X; \T) \right)^2 \right]$$
\end{frame}
%
%
\begin{frame}
  Also convienient is the \textbf{expected training error}, where the sample error is
  computed on the same dataset used to train the model:

  $$\ETE = E_{\T} \left[ \frac{1}{N} \sum_{x,y \in \T} (y - f(x; \T))^2 \right] $$
\end{frame}
%
%
\begin{frame}
  The expected training error is \textit{not} a good estimator of the expected
  out of sample error, because the modeling algorithm incentivises fitting the
  training data as closely as possilbe:
  \begin{figure}
    \includegraphics[scale=0.09]{learning_curves_by_degree}
  \end{figure}
\end{frame}
%
%
\begin{frame}
  The fact is so important, it's worth putting some math behind it:
  \begin{align*}
    \ESE &= E_{X,Y,\T} \left[ \left( Y - f(X; \T) \right)^2 \right] \\
         \only<2->{
           &= E_{\D, \T} \left[ \frac{1}{N} \sum_{x,y \in \D} (y - f(x; \T))^2
           \right] \\
         }
         \only<3>{
           &\geq E_{\D} \left[ \frac{1}{N} \sum_{x,y \in \D} (y - f(x; \D))^2 \right]
         }
         \only<4>{
           &\geq E_{\T} \left[ \frac{1}{N} \sum_{x,y \in \T} (y - f(x; \T))^2 \right]
         }
  \end{align*}

  \only<2>{
    We can replace the expectation of distributuion with the expectation of its
    sample means.
  }
  \only<3>{
    This is the definition of $f(X, \D)$, it has smaller error than all other
    possible $f$ when the sample error is evaluated on $\D$.
  }
  \only<4>{
    Changing the name of a free varaible.
  } 
\end{frame}
%
%
\begin{frame}
  Since the traning error is not a good estimate of the out of sample error, we
  need an alternative way to get at this quantity.  There are two wo major
  approaches:
  \begin{itemize}
    \item Using a \textbf{hold out set}.
    \item Using \textbf{cross validation}.
  \end{itemize}
\end{frame}
%
%
\begin{frame}
  If a dataset $\Holdout$ has participated in niether the training of the model, or
  the decision making process, then:

  $$ \frac{1}{N} \sum_{x,y \in \Holdout} (y - f(x; \T))^2 $$

  is an unbias estimate of $\ESE(\T)$ (the expected out of sample error with a
  fixed training set).
\end{frame}
%
%
\begin{frame}
  Holding out data is \textit{not} costless, as there is less data available for
  training and a smaller data set will increase the varaince of the estimated
  model:
  \begin{figure}
    \includegraphics[scale=0.08]{out_of_sample_learning_curve}
  \end{figure}
  How costly holding out data is depends on the shape of the learning curve for
  the model being estimated.
\end{frame}
%
%
\begin{frame}
  \begin{figure}
    \includegraphics[scale=0.08]{out_of_sample_learning_curve}
  \end{figure}
  This shape is affected by:
  \begin{itemize}
    \item The overall level of noise in the data (irreducible error).
    \item The complexity of the model being estimated.
  \end{itemize}
  Noisier data and more complex models may be more adversely affected by holding
  out data.
\end{frame}
%
%
\begin{frame}
  In \textbf{cross validation} the data is available is partitioned into a
  disjoint union of \textit{folds}:
  $$ {\T} = {\T}_1 \cup {\T}_2 \cup \cdots \cup {\T}_k $$
  For each $i$, the out of fold data is formed by removing the fold ${\T}_i$ from
  the training data set:
  $$ \hat{{\T}}_i = {\T}_1 \cup \cdots \cup {\T}_{i-1} \cup {\T}_{i+1}
  \cup \cdots \cup {\T}_k $$
\end{frame}
%
%
\begin{frame}
  The cross validation estimate of the out of sample error is:
   $$ \frac{1}{k} \sum_{i=1}^{k} \frac{1}{\# {\T}_i } \sum_{x,y \in
   {\T}_i} (y - f(x, \hat{{\T}}_i))^2$$ 
   Since we are randomizing over a training set, this is actually an estiamte of
   $\ESE$.
\end{frame}
%
%
\begin{frame}
  That is, while the hold out sample error is an estimate of:
  $$\ESE(\T) = E_{X,Y} \left[ \left( Y - f(X; \T) \right)^2 \right]$$
  The cross validation estimate is of:
  $$\ESE = E_{X,Y,\D} \left[ \left( Y - f(X; D) \right)^2 \right]$$
\end{frame}








